{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Script to read a list of images , convert into verctors and stored in Pinecone database followed by searching"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "844774d924c37813"
  },
  {
   "cell_type": "markdown",
   "source": [
    "--[Reference](https://blog.roboflow.com/pinecone-roboflow-inference-clip/)--"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "688658010c4f96a2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if not gpu_devices:\n",
    "    print('No GPU')\n",
    "else:\n",
    "    print('GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T02:08:10.173847400Z",
     "start_time": "2025-03-04T02:08:10.002266800Z"
    }
   },
   "id": "9f963dde5fdb8ab2",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Load environment variables from the .env file (if present)\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T02:05:50.710408Z",
     "start_time": "2025-03-04T02:03:50.797166400Z"
    }
   },
   "id": "fb7c63e173551eb0",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"../data/image\"\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T00:33:43.767802700Z",
     "start_time": "2025-03-04T00:33:43.739837300Z"
    }
   },
   "id": "c6843d5dea722150",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load CLIP model and processor.\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-03T06:17:27.656822Z",
     "start_time": "2025-03-03T06:17:24.526875900Z"
    }
   },
   "id": "a9f6ab78b8d7d2d",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to generate image vector.\n",
    "def image_to_vector(image_path):\n",
    "\n",
    "    image =  Image.open(image_path).convert('RGB')\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    print(outputs)\n",
    "    image_embedding = outputs.image_embeddings.detach().cpu().numpy()[0]\n",
    "    image.close()\n",
    "    return image_embedding\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-03T06:22:48.524302800Z",
     "start_time": "2025-03-03T06:22:48.444328100Z"
    }
   },
   "id": "15bb31e21a16597f",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def image_to_embedding(image_path):\n",
    "    # Step 1: Load the pre-trained MobileNetV2 model\n",
    "    model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "    # Step 2: Read the image from the specified path\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Check if image is loaded successfully\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "\n",
    "    # Step 3: Resize the image to 224x224 pixels (size expected by MobileNetV2)\n",
    "    image_resized = cv2.resize(image, (224, 224))\n",
    "\n",
    "    # Step 4: Convert the image to a format suitable for the model\n",
    "    image_array = img_to_array(image_resized)  # Convert to array\n",
    "    image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "    image_array = preprocess_input(image_array)  # Preprocess the image\n",
    "\n",
    "    # Step 5: Generate the embedding vector using the model\n",
    "    embedding_vector = model.predict(image_array)\n",
    "\n",
    "    return embedding_vector.flatten()  # Flatten the vector to 1D"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T02:08:57.903584400Z",
     "start_time": "2025-03-04T02:08:57.312718300Z"
    }
   },
   "id": "dbdb54368a84cad9",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def getMetadata(img):\n",
    "    metadata ={}\n",
    "    metadata[\"image_name\"] = img.filename.replace(f'{IMAGE_DIR}\\\\','')[:-4]\n",
    "    metadata[\"image_mode\"] = img.mode\n",
    "    metadata[\"image_format\"] = img.format\n",
    "    metadata[\"image_width\"], metadata[\"image_height\"] = img.size\n",
    "    metadata[\"image_category\"] = \"pizza\"\n",
    "    metadata[\"image_label\"] = metadata[\"image_name\"]\n",
    "    \n",
    "    return metadata  \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T02:25:24.224387500Z",
     "start_time": "2025-03-04T02:25:24.211374700Z"
    }
   },
   "id": "220278acfe7be4b9",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TAPAJOYTI\\AppData\\Local\\Temp\\ipykernel_144292\\2742166075.py:3: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001B[1m9406464/9406464\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 1us/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m132s\u001B[0m 132s/step\n",
      "Image embedding vector representation:\n",
      "[0.         0.99474746 0.19308046 ... 0.671874   0.5177065  0.11683978]\n",
      "Embedding vector shape: (1280,)\n"
     ]
    }
   ],
   "source": [
    "# Convert the image to an embedding vector\n",
    "embedding_vector = image_to_embedding(\"../data/image/margherita.jpg\")\n",
    "\n",
    "# Step 6: Print the embedding vector (or part of it)\n",
    "if embedding_vector is not None:\n",
    "    print(\"Image embedding vector representation:\")\n",
    "    print(embedding_vector)  # Print the entire vector\n",
    "    print(f\"Embedding vector shape: {embedding_vector.shape}\")  # Print the shape of the vector\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T02:13:27.165921900Z",
     "start_time": "2025-03-04T02:10:15.858795Z"
    }
   },
   "id": "2ed299e4cefe9198",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{\n    \"name\": \"test-image-index\",\n    \"metric\": \"cosine\",\n    \"host\": \"test-image-index-tbbbaqp.svc.aped-4627-b74a.pinecone.io\",\n    \"spec\": {\n        \"serverless\": {\n            \"cloud\": \"aws\",\n            \"region\": \"us-east-1\"\n        }\n    },\n    \"status\": {\n        \"ready\": true,\n        \"state\": \"Ready\"\n    },\n    \"vector_type\": \"dense\",\n    \"dimension\": 1280,\n    \"deletion_protection\": \"disabled\",\n    \"tags\": null\n}"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete Index\n",
    "pc.delete_index(\"test-image-index\")\n",
    "\n",
    "# Create an index called test_image\n",
    "pc.create_index(\n",
    "    name=\"test-image-index\",\n",
    "    dimension=1280,\n",
    "    spec=ServerlessSpec(\n",
    "        cloud='aws',\n",
    "        region='us-east-1'\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T02:34:39.219713900Z",
     "start_time": "2025-03-04T02:34:23.914725800Z"
    }
   },
   "id": "fcfa6d795c0cd81b",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TAPAJOYTI\\AppData\\Local\\Temp\\ipykernel_144292\\2742166075.py:3: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2s/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2s/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000020283A651C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2s/step\n"
     ]
    }
   ],
   "source": [
    "vectors =[]\n",
    "\n",
    "for fl in glob.glob(f\"{IMAGE_DIR}/*\"):\n",
    "    \n",
    "    vector = {}\n",
    "    id = str(uuid.uuid4())\n",
    "    img = Image.open(fl)\n",
    "    vector[\"id\"] = id\n",
    "    vector[\"values\"] = image_to_embedding(fl)\n",
    "    vector[\"metadata\"] = getMetadata(img)\n",
    "    vectors.append(vector)\n",
    "    img.close()\n",
    "    \n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T02:25:52.566252900Z",
     "start_time": "2025-03-04T02:25:41.553491400Z"
    }
   },
   "id": "546a1e21d82b82d",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '3a446167-c492-4083-88c5-66d3d1ac868c', 'values': array([0.00336485, 0.        , 0.30892482, ..., 3.2588277 , 0.4672242 ,\n",
      "       0.5803512 ], shape=(1280,), dtype=float32), 'metadata': {'image_name': 'classic-cheese-pizza', 'image_mode': 'RGB', 'image_format': 'JPEG', 'image_width': 1500, 'image_height': 1000, 'image_category': 'pizza', 'image_label': 'classic-cheese-pizza'}}, {'id': '4e481108-667b-49b7-b171-da8415960acd', 'values': array([0.        , 0.99474746, 0.19308046, ..., 0.671874  , 0.5177065 ,\n",
      "       0.11683978], shape=(1280,), dtype=float32), 'metadata': {'image_name': 'margherita', 'image_mode': 'RGB', 'image_format': 'JPEG', 'image_width': 225, 'image_height': 225, 'image_category': 'pizza', 'image_label': 'margherita'}}, {'id': 'a2c016e8-6f69-47a3-8df2-3c6a9b63e4cc', 'values': array([0.29437202, 0.        , 0.        , ..., 2.0288527 , 0.13304813,\n",
      "       1.608125  ], shape=(1280,), dtype=float32), 'metadata': {'image_name': 'neapolitan', 'image_mode': 'RGB', 'image_format': 'JPEG', 'image_width': 900, 'image_height': 805, 'image_category': 'pizza', 'image_label': 'neapolitan'}}]\n"
     ]
    }
   ],
   "source": [
    "print(vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T02:28:15.639668Z",
     "start_time": "2025-03-04T02:28:15.169632700Z"
    }
   },
   "id": "4a7b31b7a4553f4c",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1280,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 0}},\n",
      " 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "index = pc.Index(\"test-image-index\")\n",
    "index.upsert(vectors=vectors)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T02:35:01.774595800Z",
     "start_time": "2025-03-04T02:34:59.763017100Z"
    }
   },
   "id": "fb120721582ca0fa",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1280,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 3}},\n",
      " 'total_vector_count': 3}\n"
     ]
    }
   ],
   "source": [
    "print(index.describe_index_stats())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T02:36:55.154440400Z",
     "start_time": "2025-03-04T02:36:53.355685100Z"
    }
   },
   "id": "b365c4df08d0c6c6",
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
