{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-07T06:35:47.262739300Z",
     "start_time": "2025-03-07T06:35:47.113697700Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discovering Activation Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58d5f9922945aef7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The sigmoid and softmax functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b918294d35a1a9c3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9168]])\n"
     ]
    }
   ],
   "source": [
    "# Create a sigmoid function and apply it on input_tensor to generate a probability for a binary classification task.\n",
    "input_tensor = torch.tensor([[2.4]])\n",
    "\n",
    "# Create a sigmoid function and apply it on input_tensor\n",
    "sigmoid = nn.Sigmoid()\n",
    "probability = sigmoid(input_tensor)\n",
    "print(probability)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T03:59:06.597961400Z",
     "start_time": "2025-03-07T03:59:03.434684400Z"
    }
   },
   "id": "170fa9048154df9f",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2828e-01, 1.1698e-04, 5.7492e-01, 3.4961e-02, 1.5669e-01, 1.0503e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Create a softmax function and apply it on input_tensor to generate a probability for a multi-class classification task.\n",
    "input_tensor = torch.tensor([[1.0, -6.0, 2.5, -0.3, 1.2, 0.8]])\n",
    "\n",
    "# Create a softmax function and apply it on input_tensor\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "probabilities = softmax(input_tensor)\n",
    "print(probabilities)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T04:01:23.495729Z",
     "start_time": "2025-03-07T04:01:23.053746400Z"
    }
   },
   "id": "e2c600dc04734964",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running a Forward Pass"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54052d59c74314a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binary Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3a6117bcb183840"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0506]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a neural network that takes a 1x8 tensor as input and outputs a single value for binary classification.\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "# Implement a small neural network for binary classification\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(8, 1),\n",
    "  nn.Sigmoid()\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T04:10:09.880818100Z",
     "start_time": "2025-03-07T04:10:09.525952100Z"
    }
   },
   "id": "2681b53383546863",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### From regression to multi-class classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3430876d0f210900"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1306]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0292, 0.1107, 0.4095, 0.4506]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a 4-layer linear network that takes 11 input features from input_tensor and produces a single regression output.\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a neural network with exactly four linear layers\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, 1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)\n",
    "\n",
    "# Update the network provided to perform a multi-class classification with four outputs.\n",
    "model1 = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, 4),\n",
    "  nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "output = model1(input_tensor)\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T04:16:31.201128700Z",
     "start_time": "2025-03-07T04:16:30.877132100Z"
    }
   },
   "id": "1bea6e2cdebd1fba",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "572060651dc44e15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating one-hot encoded labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e44d5119bf92a93"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot vector using NumPy: [0 1 0]\n",
      "One-hot vector using PyTorch: tensor([0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Manually one-hot encode the ground truth label y using the provided NumPy array and save it as one_hot_numpy\n",
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "# Create the one-hot encoded vector using NumPy\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(y), num_classes=num_classes)\n",
    "\n",
    "print(\"One-hot vector using NumPy:\", one_hot_numpy)\n",
    "print(\"One-hot vector using PyTorch:\", one_hot_pytorch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T05:40:26.201190500Z",
     "start_time": "2025-03-07T05:40:17.817900700Z"
    }
   },
   "id": "166d07e7591d7eee",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculating cross entropy loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fbfdf1eee049f2e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Create the one-hot encoded vector of the ground truth label y, with 4 features (one for each class), and assign it to one_hot_label.\n",
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), num_classes=4)\n",
    "\n",
    "# Create the cross entropy loss function and store it as criterion\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Calculate the cross entropy loss using the one_hot_label vector and the scores vector, by calling the loss_function you created.\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T05:46:30.375853100Z",
     "start_time": "2025-03-07T05:46:20.927713500Z"
    }
   },
   "id": "bb06b4901f98cce7",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Derivatives"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2406efad4c2f074f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Accessing the model parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fa735ce2092a0ad"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of the first layer: Parameter containing:\n",
      "tensor([[ 0.2454,  0.1513,  0.0589, -0.1880, -0.1489, -0.1974,  0.0612, -0.2299,\n",
      "          0.1579,  0.1290, -0.2158,  0.1076, -0.2208, -0.0758,  0.0826,  0.1549],\n",
      "        [-0.1609, -0.1271,  0.1201,  0.0216, -0.1223,  0.2067, -0.0217,  0.0979,\n",
      "          0.0987,  0.0437, -0.1453,  0.2415, -0.0360,  0.1768, -0.1418, -0.1298],\n",
      "        [-0.1864, -0.0895, -0.1360, -0.1801, -0.1693, -0.0127, -0.0830,  0.0159,\n",
      "          0.0544,  0.0818,  0.0633,  0.1599,  0.1866,  0.1243, -0.2415,  0.1278],\n",
      "        [-0.2293, -0.1717,  0.0998, -0.0601, -0.2053, -0.0783,  0.2459, -0.1927,\n",
      "         -0.1767,  0.0207, -0.0043, -0.0339,  0.0746,  0.1216, -0.1386, -0.2237],\n",
      "        [ 0.1422, -0.1353, -0.2238, -0.0410, -0.0755, -0.1696, -0.0475,  0.0203,\n",
      "         -0.1766, -0.0026, -0.1624,  0.0088, -0.2401,  0.0663, -0.0911, -0.0575],\n",
      "        [ 0.1247,  0.0772,  0.0193, -0.1908,  0.0413,  0.0069,  0.1467, -0.2308,\n",
      "         -0.1282,  0.2292,  0.0425,  0.2248, -0.0205, -0.1609,  0.1004,  0.0860],\n",
      "        [-0.1832,  0.1023,  0.1980,  0.1990, -0.1788, -0.2180,  0.0931, -0.0125,\n",
      "          0.0753, -0.1774,  0.1079,  0.0571, -0.0623,  0.0744,  0.1560,  0.2064],\n",
      "        [-0.2062,  0.0499,  0.0606,  0.2186,  0.0287,  0.0268, -0.1136, -0.2295,\n",
      "          0.2344, -0.0295, -0.0118,  0.0131,  0.1782,  0.2150,  0.2297,  0.0371]],\n",
      "       requires_grad=True)\n",
      "Bias of the second layer: Parameter containing:\n",
      "tensor([-0.1579,  0.2194], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Linear(8, 2)\n",
    "                     )\n",
    "# Access the weight parameter of the first linear layer.\n",
    "weight_0 = model[0].weight\n",
    "print(\"Weight of the first layer:\", weight_0)\n",
    "\n",
    "# Access the bias parameter of the second linear layer.\n",
    "bias_1 = model[1].bias\n",
    "print(\"Bias of the second layer:\", bias_1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T05:57:22.206699900Z",
     "start_time": "2025-03-07T05:57:21.862160600Z"
    }
   },
   "id": "a3603c9c11929abe",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Updating the weights manually"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb90571045ff9b42"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Learning rate is typically smalllr = 0.001\n",
    "lr = 0.001\n",
    "y = [1]\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9,-1,4,8,-2,0,-10,13,7]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "target = F.one_hot(torch.tensor(y), num_classes=2)\n",
    "\n",
    "\n",
    "# Implement a neural network with exactly four linear layers\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(16, 8),\n",
    "  nn.Linear(8, 4),\n",
    "  nn.Linear(4, 2)\n",
    ")\n",
    "\n",
    "pred = model(input_tensor)\n",
    "\n",
    "# Calculate the loss and gradients\n",
    "#criterion = CrossEntropyLoss()\n",
    "#loss = criterion(prediction.double(), one_hot_label.double())\n",
    "#loss.backward()\n",
    "\n",
    "\n",
    "# # Create the gradient variables by accessing the local gradients of each weight tensor.\n",
    "# weight0 = model[0].weight\n",
    "# weight1 = model[1].weight\n",
    "# weight2 = model[2].weight\n",
    "# \n",
    "# # Access the gradients of the weight of each linear layer\n",
    "# grads0 = weight0.grad\n",
    "# grads1 = weight1.grad\n",
    "# grads2 = weight2.grad\n",
    "# \n",
    "# # Update the weights using the learning rate and the gradients\n",
    "# weight0 = weight0 - lr * grads0\n",
    "# weight1 = weight1 - lr * grads1\n",
    "# weight2 = weight2 - lr * grads2\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T06:45:52.026356100Z",
     "start_time": "2025-03-07T06:45:52.002950600Z"
    }
   },
   "id": "e70165b9a96ff77b",
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using the PyTorch optimizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5107f908365b9b87"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Use optim to create an SGD optimizer with a learning rate of your choice (must be less than one) for the model provided.\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "loss = criterion(pred.double(), target.double())\n",
    "loss.backward()\n",
    "\n",
    "# Update the model's parameters using the optimizer.\n",
    "optimizer.step()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T06:45:57.557507700Z",
     "start_time": "2025-03-07T06:45:57.542512500Z"
    }
   },
   "id": "5f037ee60779e1df",
   "execution_count": 61
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
